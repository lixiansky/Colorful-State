import os
import time
import random
import json
import requests
from datetime import datetime
from playwright.sync_api import sync_playwright
from playwright_stealth import stealth_sync
from bs4 import BeautifulSoup
import psycopg2
from psycopg2.extras import Json
from dotenv import load_dotenv
from openai import OpenAI
import cv2
import tempfile
import base64
import shutil

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®
USERS_STR = os.environ.get('TWITTER_USERS', 'elonmusk')
USERS = [u.strip() for u in USERS_STR.split(',') if u.strip()]

# DeepSeek API é…ç½®
DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY')
DEEPSEEK_BASE_URL = os.environ.get('DEEPSEEK_BASE_URL', 'https://api.deepseek.com')

# Neon Database é…ç½®
DATABASE_URL = os.environ.get('DATABASE_URL')

# è¿è¡Œæ¨¡å¼é…ç½®
LOOP_MODE = os.environ.get('LOOP_MODE', 'false').lower() == 'true'
INTERVAL = int(os.environ.get('LOOP_INTERVAL', '600'))  # é»˜è®¤ 10 åˆ†é’Ÿ

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INSTANCES_FILE = os.path.join(BASE_DIR, 'instances.json')

# Nitter å®ä¾‹åˆ—è¡¨ï¼ˆä¼˜å…ˆä½¿ç”¨æ”¯æŒè§†é¢‘çš„å®ä¾‹ï¼‰
NITTER_INSTANCES = [
    'https://xcancel.com',  # æ”¯æŒè§†é¢‘ (source tag)
    'https://nitter.privacyredirect.com',  # æ”¯æŒè§†é¢‘ (data-url)
    "https://nitter.net",
    "https://nitter.catsarch.com",
    "https://nitter.tiekoetter.com",
    "https://nitter.poast.org",
    "https://nuku.trabun.org",
    "https://lightbrd.com",
    "https://nitter.space"
]

def get_random_user_agent():
    """è·å–éšæœº User-Agent"""
    ua_list = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/121.0.0.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0"
    ]
    return random.choice(ua_list)

def load_instances():
    """ä»æœ¬åœ°ç¼“å­˜åŠ è½½å¥åº·çš„ Nitter å®ä¾‹"""
    if os.path.exists(INSTANCES_FILE):
        try:
            with open(INSTANCES_FILE, 'r', encoding='utf-8') as f:
                instances = json.load(f)
                if instances and isinstance(instances, list):
                    print(f"[ç³»ç»Ÿ] æˆåŠŸä»æœ¬åœ°ç¼“å­˜åŠ è½½ {len(instances)} ä¸ªå®ä¾‹")
                    return instances
        except Exception as e:
            print(f"[ç³»ç»Ÿ] åŠ è½½å®ä¾‹ç¼“å­˜å¤±è´¥: {e}")
    
    print("[ç³»ç»Ÿ] ç¼“å­˜ä¸å­˜åœ¨æˆ–æŸåï¼Œé‡‡ç”¨å†…ç½®å…œåº•å®ä¾‹åˆ—è¡¨")
    return NITTER_INSTANCES

def upload_to_imgbb(image_path_or_url):
    """
    ä¸Šä¼ å›¾ç‰‡åˆ° ImgBB å›¾åºŠ
    æ”¯æŒæœ¬åœ°æ–‡ä»¶è·¯å¾„æˆ–ç½‘ç»œ URL
    éœ€è¦é…ç½®ç¯å¢ƒå˜é‡: IMGBB_API_KEY
    """
    api_key = os.environ.get('IMGBB_API_KEY', '').strip()
    if not api_key:
        print("[å›¾åºŠ] ImgBB æœªé…ç½® API Key, æ— æ³•ä¸Šä¼ ")
        return None
    
    try:
        # å‡†å¤‡å›¾ç‰‡æ•°æ®
        img_base64 = None
        
        # åˆ¤æ–­æ˜¯æœ¬åœ°æ–‡ä»¶è¿˜æ˜¯ URL
        if os.path.exists(image_path_or_url):
            print(f"[å›¾åºŠ] æ­£åœ¨ä¸Šä¼ æœ¬åœ°æ–‡ä»¶: {image_path_or_url}")
            with open(image_path_or_url, "rb") as image_file:
                img_base64 = base64.b64encode(image_file.read()).decode('utf-8')
        else:
            print(f"[å›¾åºŠ] æ­£åœ¨ä» {image_path_or_url} ä¸‹è½½å›¾ç‰‡...")
            img_response = requests.get(image_path_or_url, timeout=30, headers={
                'User-Agent': get_random_user_agent(),
                'Referer': 'https://twitter.com/'
            })
            img_response.raise_for_status()
            img_base64 = base64.b64encode(img_response.content).decode('utf-8')
        
        # ä¸Šä¼ åˆ° ImgBB
        print("[å›¾åºŠ] æ­£åœ¨ä¸Šä¼ åˆ° ImgBB...")
        upload_response = requests.post(
            'https://api.imgbb.com/1/upload',
            data={
                'key': api_key,
                'image': img_base64
            },
            timeout=30
        )
        result = upload_response.json()
        
        if result.get('success'):
            url = result['data']['url']
            print(f"[å›¾åºŠ] ImgBB ä¸Šä¼ æˆåŠŸ: {url}")
            return url
        else:
            print(f"[å›¾åºŠ] ImgBB ä¸Šä¼ å¤±è´¥: {result}")
            return None
    except Exception as e:
        print(f"[å›¾åºŠ] ImgBB ä¸Šä¼ å¼‚å¸¸: {e}")
        return None

def extract_video_frame(video_url):
    """
    æå–è§†é¢‘ä¸­é—´å¸§å¹¶ä¸Šä¼ åˆ°å›¾åºŠ
    è¿”å›: å›¾åºŠ URL æˆ– None
    """
    if not video_url:
        return None
        
    temp_video = None
    temp_image = None
    
    try:
        # 1. åˆ¤æ–­æ˜¯å¦ä¸º M3U8 æµåª’ä½“
        is_m3u8 = '.m3u8' in video_url.lower()
        
        if is_m3u8:
            print(f"[è§†é¢‘] æ£€æµ‹åˆ° M3U8 æµåª’ä½“ï¼Œå°è¯•ç›´æ¥åœ¨çº¿è¯»å–: {video_url[:60]}...")
            # å¯¹äº M3U8ï¼Œç›´æ¥å°† URL ä¼ ç»™ OpenCV (éœ€è¦ FFmpeg æ”¯æŒ)
            cap = cv2.VideoCapture(video_url)
        else:
            print(f"[è§†é¢‘] æ­£åœ¨ä¸‹è½½è§†é¢‘ä»¥æå–å°é¢: {video_url[:60]}...")
            # 1. ä¸‹è½½è§†é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            headers = {
                "User-Agent": get_random_user_agent()
            }
            response = requests.get(video_url, stream=True, timeout=60, headers=headers)
            if response.status_code != 200:
                print(f"[è§†é¢‘] ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
                
            fd, temp_video = tempfile.mkstemp(suffix='.mp4')
            with os.fdopen(fd, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # æ‰“å¼€ä¸´æ—¶æ–‡ä»¶
            cap = cv2.VideoCapture(temp_video)

        # 2. ä½¿ç”¨ OpenCV æå–ä¸­é—´å¸§
        if not cap.isOpened():
            print(f"[è§†é¢‘] æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            return None

        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if frame_count <= 0:
            # å°è¯•è¯»å–ç¬¬ä¸€å¸§
            frame_count = 1
            
        #å³ä¾¿åªæœ‰å‡ å¸§ï¼Œå–ä¸­é—´æˆ–ç¬¬ä¸€å¸§
        middle_frame = max(0, frame_count // 2)
        cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame)

        ret, frame = cap.read()
        cap.release()
        
        if not ret:
            print(f"[è§†é¢‘] è¯»å–è§†é¢‘å¸§å¤±è´¥")
            return None
            
        # 3. ä¿å­˜å¸§åˆ°ä¸´æ—¶å›¾ç‰‡
        fd_img, temp_image = tempfile.mkstemp(suffix='.jpg')
        os.close(fd_img) # mkstemp è¿”å›çš„ fd éœ€è¦å…³é—­ï¼Œå› ä¸º cv2.imwrite ä½¿ç”¨è·¯å¾„
        
        cv2.imwrite(temp_image, frame)
        print(f"[è§†é¢‘] æˆåŠŸæå–å¸§åˆ°: {temp_image}")
        
        # 4. ä¸Šä¼ åˆ°å›¾åºŠ
        img_url = upload_to_imgbb(temp_image)
        return img_url
        
    except Exception as e:
        print(f"[è§†é¢‘] æå–å°é¢å¼‚å¸¸: {e}")
        return None
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_video and os.path.exists(temp_video):
            try:
                os.remove(temp_video)
            except:
                pass
        if temp_image and os.path.exists(temp_image):
            try:
                os.remove(temp_image)
            except:
                pass

def get_original_image_url(nitter_url):
    """å°è¯•ä» Nitter çš„ä»£ç† URL ä¸­è¿˜åŸå‡º Twitter/X çš„åŸå§‹å›¾ç‰‡åœ°å€"""
    import urllib.parse
    import re
    try:
        if 'pbs.twimg.com' in nitter_url:
            return nitter_url
            
        # å¤„ç† hex ç¼–ç 
        if '/pic/enc/' in nitter_url:
            enc_part = nitter_url.split('/pic/enc/')[-1].split('?')[0]
            try:
                decoded = bytes.fromhex(enc_part).decode('utf-8')
                if 'pbs.twimg.com' in decoded:
                    return decoded
            except:
                pass

        # å¤„ç†æ ‡å‡† Nitter è·¯å¾„
        path = urllib.parse.unquote(nitter_url)
        
        if '/media/' in path:
            media_part = path.split('/media/')[-1].split('?')[0]
            if '.' in media_part:
                media_id, ext = media_part.rsplit('.', 1)
                ext = ext.split('&')[0].split('?')[0]
                return f"https://pbs.twimg.com/media/{media_id}?format={ext}&name=large"

        if 'pbs.twimg.com' in path:
            match = re.search(r'(pbs\.twimg\.com/media/[^?&]+)', path)
            if match:
                return "https://" + match.group(1)

    except Exception as e:
        print(f"[å›¾ç‰‡è§£æ] è¿˜åŸ URL å¤±è´¥ {nitter_url}: {e}")
        
    return nitter_url

def check_url_accessibility(url):
    """
    æ£€æŸ¥ URL æ˜¯å¦å¯è®¿é—® (è¿”å› 200 OK)
    å¢åŠ æ£€æŸ¥:
    1. æ˜¯å¦ä¸ºä½æ¸…ç¼©ç•¥å›¾ (name=small) -> æ‹’ç»
    2. Content-Type æ˜¯å¦ä¸ºå›¾ç‰‡ (image/*) -> æ‹’ç» HTML (404/503 é¡µé¢çš„ä¼ªè£…)
    """
    if not url:
        return False
        
    try:
        from urllib.parse import unquote
        decoded_url = unquote(url)
        
        # è§„åˆ™1: æ‹’ç»ä½æ¸…ç¼©ç•¥å›¾ï¼Œå¼ºåˆ¶é‡æ–°ç”Ÿæˆ
        # æ£€æŸ¥åŸå§‹ URL å’Œè§£ç åçš„ URL
        if 'name=small' in url or 'name=small' in decoded_url:
            print(f"[è®¿é—®æ£€æŸ¥] âš ï¸ æ‹’ç»ä½æ¸…ç¼©ç•¥å›¾ (name=small): {url[:60]}...")
            return False

        headers = {
            "User-Agent": get_random_user_agent()
        }
        # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ (5ç§’)ï¼Œä½¿ç”¨ stream=True åªè¯»å–å“åº”å¤´
        response = requests.get(url, stream=True, timeout=10, headers=headers)
        
        if response.status_code == 200:
            # è§„åˆ™2: æ£€æŸ¥ Content-Type
            content_type = response.headers.get('Content-Type', '').lower()
            if not content_type.startswith('image/'):
                print(f"[è®¿é—®æ£€æŸ¥] âš ï¸ URL è¿”å›éå›¾ç‰‡ç±»å‹ ({content_type}): {url[:60]}...")
                return False
                
            return True
        else:
            print(f"[è®¿é—®æ£€æŸ¥] URL è¿”å›é 200 çŠ¶æ€ç : {response.status_code} - {url[:60]}...")
            return False
    except Exception as e:
        print(f"[è®¿é—®æ£€æŸ¥] è®¿é—®å¤±è´¥: {url[:60]}... é”™è¯¯: {e}")
        return False

def scrape_nitter_with_playwright(target, dynamic_instances=None):
    """ä½¿ç”¨ Playwright æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—® Nitter å¹¶æŠ“å–æœ€æ–°æ¨æ–‡"""
    is_search = target.startswith('search:')
    keyword = target[7:] if is_search else target
    
    instances = dynamic_instances if dynamic_instances else NITTER_INSTANCES.copy()
    
    # éšæœºæ‰“ä¹±å®ä¾‹é¡ºåº
    if len(instances) > 5:
        top_5 = instances[:5]
        random.shuffle(top_5)
        others = instances[5:]
        random.shuffle(others)
        instances = top_5 + others
    else:
        random.shuffle(instances)
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        
        for instance in instances:
            try:
                context = browser.new_context(
                    user_agent=get_random_user_agent(),
                    viewport={'width': 1280, 'height': 720}
                )
                page = context.new_page()
                stealth_sync(page)
                
                if is_search:
                    url = f"{instance.rstrip('/')}/search?f=tweets&q={requests.utils.quote(keyword)}"
                else:
                    url = f"{instance.rstrip('/')}/{keyword}"
                
                print(f"[{target}] æ­£åœ¨åŠ è½½: {url}")
                
                try:
                    response = page.goto(url, wait_until="networkidle", timeout=45000)
                    if response and response.status == 403:
                        print(f"[{target}] è®¿é—® {instance} è¢«æ‹’ (403 Forbidden)")
                        context.close()
                        continue
                except Exception as e:
                    print(f"[{target}] åŠ è½½ {instance} è¶…æ—¶æˆ–å¤±è´¥: {e}")
                    context.close()
                    continue
                
                # æ™ºèƒ½ç­‰å¾…æµè§ˆå™¨éªŒè¯
                challenge_keywords = ["Verifying your browser", "Just a moment", "Checking your browser"]
                for i in range(5):
                    content = page.content()
                    if any(kw in content for kw in challenge_keywords):
                        print(f"[{target}] æ£€æµ‹åˆ°æµè§ˆå™¨éªŒè¯ ({i+1}/5)ï¼Œå°è¯•ç­‰å¾…...")
                        page.wait_for_timeout(5000)
                    else:
                        break
                
                soup = BeautifulSoup(page.content(), 'html.parser')
                items = soup.select('.timeline-item')
                
                if not items:
                    print(f"[{target}] åœ¨å®ä¾‹ {instance} ä¸Šæœªå‘ç°æ¨æ–‡å†…å®¹")
                    context.close()
                    continue
                
                # æ‰«æå‰ 8 æ¡æ¨æ–‡
                valid_tweets = []
                for item in items[:8]:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç½®é¡¶æ¨æ–‡
                    is_pinned = item.select_one('.pinned') is not None
                    if is_pinned:
                        print(f"[{target}] å‘ç°ç½®é¡¶æ¨æ–‡ï¼Œè·³è¿‡")
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è½¬å‘
                    is_retweet = item.select_one('.retweet-header') is not None

                    # æå–å›¾ç‰‡
                    images = []
                    img_els = item.select('.attachment.image img, .tweet-image img, .still-image img, .attachments img')
                    for img in img_els:
                        if any(c in str(img.parent.get('class', [])) for c in ['avatar', 'profile']):
                            continue
                            
                        src = img.get('src', '')
                        if src:
                            if src.startswith('//'):
                                full_src = 'https:' + src
                            elif src.startswith('/'):
                                full_src = instance.rstrip('/') + src
                            else:
                                full_src = src
                            
                            full_src = get_original_image_url(full_src)
                            
                            if 'emoji' in src.lower() or 'hashtag_click' in src:
                                continue
                                
                            images.append(full_src)

                    # æå–è§†é¢‘
                    video_url = None
                    try:
                        video_tag = item.select_one('video')
                        
                        if video_tag:
                            # æ–¹æ³•1: æ£€æŸ¥ data-url å±æ€§ï¼ˆNitter çš„ä¸»è¦æ–¹å¼ï¼‰
                            data_url = video_tag.get('data-url', '')
                            if data_url:
                                # data-url å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„æˆ–åŒ…å«ç¼–ç çš„ URL
                                if data_url.startswith('/video/'):
                                    # æ ¼å¼: /video/ID/https%3A%2F%2F...
                                    # æå–å®é™…çš„è§†é¢‘ URL
                                    parts = data_url.split('/', 3)
                                    if len(parts) > 3:
                                        from urllib.parse import unquote
                                        encoded_url = parts[3]
                                        video_url = unquote(encoded_url)
                                        print(f"[{target}] æ‰¾åˆ°è§†é¢‘ (data-url): {video_url[:80]}...")
                                elif data_url.startswith('//'):
                                    video_url = 'https:' + data_url
                                    print(f"[{target}] æ‰¾åˆ°è§†é¢‘ (data-url): {video_url[:80]}...")
                                elif data_url.startswith('/'):
                                    video_url = instance.rstrip('/') + data_url
                                    print(f"[{target}] æ‰¾åˆ°è§†é¢‘ (data-url): {video_url[:80]}...")
                                else:
                                    video_url = data_url
                                    print(f"[{target}] æ‰¾åˆ°è§†é¢‘ (data-url): {video_url[:80]}...")
                            
                            # æ–¹æ³•2: æ£€æŸ¥ src å±æ€§
                            if not video_url:
                                v_src = video_tag.get('src', '')
                                if v_src:
                                    if v_src.startswith('//'):
                                        video_url = 'https:' + v_src
                                    elif v_src.startswith('/'):
                                        video_url = instance.rstrip('/') + v_src
                                    else:
                                        video_url = v_src
                                    print(f"[{target}] æ‰¾åˆ°è§†é¢‘ (src): {video_url[:80]}...")
                            
                            # æå–å°é¢å›¾
                            poster = video_tag.get('poster', '')
                            if poster:
                                if poster.startswith('//'):
                                    full_poster = 'https:' + poster
                                elif poster.startswith('/'):
                                    full_poster = instance.rstrip('/') + poster
                                else:
                                    full_poster = poster
                                full_poster = get_original_image_url(full_poster)
                                
                                # éªŒè¯å°é¢å›¾æ˜¯å¦å¯è®¿é—®
                                if check_url_accessibility(full_poster):
                                    if full_poster not in images:
                                        images.append(full_poster)
                                        poster_added = True
                                else:
                                    print(f"[{target}] âš ï¸ å°é¢å›¾æ— æ³•è®¿é—®ï¼Œè·³è¿‡: {full_poster}")
                        
                        # æ–¹æ³•3: æ£€æŸ¥ video source æ ‡ç­¾
                        if not video_url:
                            video_source = item.select_one('video source')
                            if video_source:
                                v_src = video_source.get('src', '')
                                if v_src:
                                    if v_src.startswith('//'):
                                        video_url = 'https:' + v_src
                                    elif v_src.startswith('/'):
                                        video_url = instance.rstrip('/') + v_src
                                    else:
                                        video_url = v_src
                                    print(f"[{target}] æ‰¾åˆ°è§†é¢‘ (source): {video_url[:80]}...")
                        
                        # æ–¹æ³•4: æŸ¥æ‰¾è§†é¢‘é“¾æ¥
                        if not video_url:
                            video_links = item.select('a[href*=".mp4"], a[href*=".m3u8"]')
                            for link in video_links:
                                href = link.get('href', '')
                                if href and ('.mp4' in href or '.m3u8' in href):
                                    if href.startswith('//'):
                                        video_url = 'https:' + href
                                    elif href.startswith('/'):
                                        video_url = instance.rstrip('/') + href
                                    else:
                                        video_url = href
                                    print(f"[{target}] æ‰¾åˆ°è§†é¢‘ (link): {video_url[:80]}...")
                                    break
                        
                        # [é€šç”¨é€»è¾‘] å¦‚æœæ²¡æœ‰å°é¢å›¾(æˆ–å®ƒæ˜¯ç©ºçš„)ï¼Œä¸”æœ‰è§†é¢‘é“¾æ¥ï¼Œå°è¯•ç”Ÿæˆ
                        # æ­¤æ—¶ poster_added å˜é‡å¯èƒ½æœªå®šä¹‰(å¦‚æœæ²¡è¿›æ–¹æ³•1)ï¼Œéœ€è¦é‡æ–°åˆ¤æ–­
                        has_poster = False
                        # ç®€å•æ£€æŸ¥ images åˆ—è¡¨é‡Œæ˜¯å¦å·²æœ‰å›¾ç‰‡ï¼Œä¸”è§†é¢‘å­˜åœ¨
                        # æ³¨æ„ï¼šæ¨æ–‡å¯èƒ½æœ‰å¤šå¼ å›¾ï¼Œè¿™é‡Œå‡å®šå¦‚æœ images ä¸ºç©ºæˆ–è€…åªæœ‰å¤´åƒï¼ˆå·²è¿‡æ»¤ï¼‰ï¼Œä¸”æœ‰è§†é¢‘ï¼Œåˆ™éœ€è¦å°é¢
                        # æ›´ä¸¥è°¨çš„åšæ³•æ˜¯ï¼šå¦‚æœ poster_added ä¸º Trueï¼Œæˆ–è€… images ä¸ä¸ºç©º
                        
                        # é‡æ–°è®¡ç®— poster_added çŠ¶æ€
                        if 'poster_added' not in locals():
                            poster_added = False
                            
                        if not poster_added and video_url:
                            # å†æ¬¡æ£€æŸ¥ images åˆ—è¡¨ï¼Œé˜²æ­¢é‡å¤æ·»åŠ 
                            if not images: 
                                print(f"[{target}] âš ï¸ è§†é¢‘æ²¡æœ‰å°é¢å›¾ï¼Œå°è¯•ç”Ÿæˆ...")
                                generated_poster = extract_video_frame(video_url)
                                if generated_poster:
                                    if generated_poster not in images:
                                        images.append(generated_poster)
                                    print(f"[{target}] âœ… è§†é¢‘å°é¢ç”ŸæˆæˆåŠŸ: {generated_poster}")
                        
                        # å¦‚æœä»æœªæ‰¾åˆ°ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯
                        if not video_url:
                            has_video_indicator = item.select_one('.video-container, .video-overlay, video')
                            if has_video_indicator:
                                print(f"[{target}] æ£€æµ‹åˆ°è§†é¢‘ä½†æœªèƒ½æå– URL")
                                
                    except Exception as e:
                        print(f"[{target}] è§†é¢‘æå–å¼‚å¸¸: {e}")

                    # æå–å…³é”®ä¿¡æ¯
                    content_el = item.select_one('.tweet-content')
                    link_el = item.select_one('.tweet-link')
                    date_el = item.select_one('.tweet-date a')
                    author_el = item.select_one('.username')

                    if not content_el or not link_el:
                        continue

                    # æå–æ¨æ–‡ ID
                    link_href = link_el.get('href', '')
                    tweet_id = link_href.split('/status/')[-1].split('#')[0] if '/status/' in link_href else link_href

                    tweet_data = {
                        'content': content_el.get_text(strip=True),
                        'link': instance.rstrip('/') + link_href,
                        'published': date_el.get('title', '') if date_el else 'Unknown Time',
                        'author': author_el.get_text(strip=True) if author_el else keyword,
                        'guid': tweet_id,
                        'is_retweet': is_retweet,
                        'images': images,
                        'video_url': video_url
                    }
                    valid_tweets.append(tweet_data)
                    
                    if len(valid_tweets) >= 1:
                        break

                if valid_tweets:
                    tweet = valid_tweets[0]
                    retweet_tag = " [è½¬å‘]" if tweet['is_retweet'] else ""
                    print(f"[{target}] æˆåŠŸä» {instance} æŠ“å–{retweet_tag}æ¨æ–‡: {tweet['guid']}")
                    context.close()
                    browser.close()
                    return tweet

                print(f"[{target}] {instance} é¡µé¢ä¸Šæœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„éç½®é¡¶æ¨æ–‡")
                context.close()

            except Exception as e:
                print(f"[{target}] è®¿é—® {instance} å‡ºé”™: {e}")
                continue
        
        browser.close()
    return None

def translate_with_deepseek(text):
    """ä½¿ç”¨ DeepSeek API ç¿»è¯‘æ–‡æœ¬ä¸ºä¸­æ–‡"""
    if not text or not text.strip():
        return ""
    
    if not DEEPSEEK_API_KEY:
        print("[ç¿»è¯‘] DeepSeek API Key æœªé…ç½®ï¼Œè·³è¿‡ç¿»è¯‘")
        return None
    
    try:
        client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL
        )
        
        print(f"[ç¿»è¯‘] æ­£åœ¨ç¿»è¯‘æ–‡æœ¬...")
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ï¼Œè¯·å°†ç”¨æˆ·æä¾›çš„æ–‡æœ¬ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ã€‚åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹ã€‚"},
                {"role": "user", "content": f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ï¼š\n\n{text}"}
            ],
            temperature=1.3,  # å®˜æ–¹æ¨èç¿»è¯‘åœºæ™¯å‚æ•°
            max_tokens=2000
        )
        
        translated = response.choices[0].message.content.strip()
        print(f"[ç¿»è¯‘] ç¿»è¯‘æˆåŠŸ")
        return translated
        
    except Exception as e:
        print(f"[ç¿»è¯‘] DeepSeek API è°ƒç”¨å¤±è´¥: {e}")
        return None

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    if not DATABASE_URL:
        raise ValueError("DATABASE_URL ç¯å¢ƒå˜é‡æœªé…ç½®")
    
    return psycopg2.connect(DATABASE_URL)

def save_tweet_to_db(tweet):
    """ä¿å­˜æ¨æ–‡åˆ°æ•°æ®åº“"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # ç¿»è¯‘æ¨æ–‡å†…å®¹
        content_zh = translate_with_deepseek(tweet['content'])
        
        # è§£æå‘å¸ƒæ—¶é—´
        published_at = None
        if tweet.get('published') and tweet['published'] != 'Unknown Time':
            try:
                # å°è¯•è§£ææ—¶é—´æ ¼å¼
                published_at = datetime.strptime(tweet['published'], '%b %d, %Y Â· %I:%M %p %Z')
            except:
                try:
                    published_at = datetime.fromisoformat(tweet['published'])
                except:
                    print(f"[æ•°æ®åº“] æ— æ³•è§£ææ—¶é—´æ ¼å¼: {tweet['published']}")
        
        # è®°å½•è§†é¢‘ URL ä¿¡æ¯
        video_url = tweet.get('video_url')
        if video_url:
            print(f"[æ•°æ®åº“] ğŸ“¹ å‡†å¤‡ä¿å­˜è§†é¢‘ URL: {video_url[:100]}...")
        else:
            print(f"[æ•°æ®åº“] âš ï¸  æ¨æ–‡æ²¡æœ‰è§†é¢‘ URL")
        
        # æ’å…¥æˆ–æ›´æ–°æ¨æ–‡
        cursor.execute("""
            INSERT INTO tweets (tweet_id, author, content, content_zh, published_at, is_retweet, images, video_url, source_url)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (tweet_id) 
            DO UPDATE SET
                content = EXCLUDED.content,
                content_zh = EXCLUDED.content_zh,
                images = EXCLUDED.images,
                video_url = EXCLUDED.video_url,
                source_url = EXCLUDED.source_url,
                updated_at = CURRENT_TIMESTAMP
            RETURNING id;
        """, (
            tweet['guid'],
            tweet['author'],
            tweet['content'],
            content_zh,
            published_at,
            tweet.get('is_retweet', False),
            Json(tweet.get('images', [])),
            video_url,
            tweet.get('link')
        ))
        
        tweet_db_id = cursor.fetchone()[0]
        conn.commit()
        
        # éªŒè¯ä¿å­˜çš„æ•°æ®
        cursor.execute("SELECT video_url FROM tweets WHERE id = %s;", (tweet_db_id,))
        saved_video_url = cursor.fetchone()[0]
        
        if saved_video_url:
            print(f"[æ•°æ®åº“] âœ… æ¨æ–‡å·²ä¿å­˜ï¼Œè§†é¢‘ URL å·²å­˜å‚¨: {saved_video_url[:100]}...")
        else:
            print(f"[æ•°æ®åº“] âœ… æ¨æ–‡å·²ä¿å­˜ (ID: {tweet_db_id}, Tweet ID: {tweet['guid']})")
            if video_url:
                print(f"[æ•°æ®åº“] âš ï¸  è­¦å‘Š: è§†é¢‘ URL æœªèƒ½ä¿å­˜åˆ°æ•°æ®åº“ï¼")
        
        cursor.close()
        conn.close()
        
        return True
        
    except Exception as e:
        print(f"[æ•°æ®åº“] âŒ ä¿å­˜æ¨æ–‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def parse_tweet_url(url):
    """
    è§£ææ¨æ–‡ URL æå–ç”¨æˆ·åå’Œæ¨æ–‡ ID
    æ”¯æŒæ ¼å¼:
    - https://x.com/user/status/123456
    - https://twitter.com/user/status/123456
    """
    import re
    pattern = r'(?:x\.com|twitter\.com)/([^/]+)/status/(\d+)'
    match = re.search(pattern, url)
    if match:
        return {
            'username': match.group(1),
            'tweet_id': match.group(2),
            'url': url
        }
    return None

def load_tweet_urls_from_file(filepath='tweets.txt'):
    """ä»æ–‡ä»¶è¯»å–æ¨æ–‡ URL åˆ—è¡¨"""
    if not os.path.exists(filepath):
        return []
    
    urls = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if line and not line.startswith('#'):
                    urls.append(line)
        
        if urls:
            print(f"[è¯»å–é…ç½®] ä» {filepath} è¯»å–åˆ° {len(urls)} æ¡æ¨æ–‡ URL")
        return urls
    except Exception as e:
        print(f"[è¯»å–é…ç½®] è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return []

def check_tweet_status(tweet_urls):
    """
    æ£€æŸ¥æ¨æ–‡åˆ—è¡¨çš„æŠ“å–çŠ¶æ€
    è¿”å›: (å·²æŠ“å–åˆ—è¡¨, å¾…æŠ“å–åˆ—è¡¨)
    """
    if not tweet_urls:
        return [], []
    
    parsed_tweets = []
    for url in tweet_urls:
        parsed = parse_tweet_url(url)
        if parsed:
            parsed_tweets.append(parsed)
        else:
            print(f"[URLè§£æ] æ— æ•ˆçš„æ¨æ–‡ URL: {url}")
    
    if not parsed_tweets:
        return [], []
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        tweet_ids = [t['tweet_id'] for t in parsed_tweets]
        placeholders = ','.join(['%s'] * len(tweet_ids))
        
        cursor.execute(f"""
            SELECT tweet_id, author, created_at, content_zh IS NOT NULL as has_translation
            FROM tweets
            WHERE tweet_id IN ({placeholders})
        """, tweet_ids)
        
        scraped_dict = {row[0]: {'author': row[1], 'scraped_at': row[2], 'has_translation': row[3]} for row in cursor.fetchall()}
        
        scraped = []
        pending = []
        
        for tweet in parsed_tweets:
            if tweet['tweet_id'] in scraped_dict:
                info = scraped_dict[tweet['tweet_id']]
                scraped.append({
                    **tweet,
                    'scraped_at': info['scraped_at'],
                    'has_translation': info['has_translation']
                })
            else:
                pending.append(tweet)
        
        cursor.close()
        conn.close()
        
        return scraped, pending
        
    except Exception as e:
        print(f"[çŠ¶æ€æ£€æŸ¥] æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        # å¤±è´¥æ—¶ï¼Œå…¨éƒ¨è§†ä¸ºå¾…æŠ“å–
        return [], parsed_tweets

def print_status_report(scraped, pending):
    """æ‰“å°çŠ¶æ€æŠ¥å‘Š"""
    total = len(scraped) + len(pending)
    
    print(f"\n{'='*60}")
    print(f"[çŠ¶æ€æ£€æŸ¥] é…ç½®äº† {total} æ¡æ¨æ–‡ URL")
    print(f"{'='*60}")
    
    if scraped:
        print(f"\nâœ… å·²æŠ“å–: {len(scraped)} æ¡")
        for tweet in scraped:
            trans_status = "âœ“ å·²ç¿»è¯‘" if tweet['has_translation'] else "âœ— æœªç¿»è¯‘"
            scraped_time = tweet['scraped_at'].strftime('%Y-%m-%d %H:%M') if tweet['scraped_at'] else 'æœªçŸ¥æ—¶é—´'
            print(f"   - {tweet['url']}")
            print(f"     @{tweet['username']} | {scraped_time} | {trans_status}")
    
    if pending:
        print(f"\nâ³ å¾…æŠ“å–: {len(pending)} æ¡")
        for tweet in pending:
            print(f"   - {tweet['url']}")
    
    print(f"\n{'='*60}\n")

def scrape_tweet_by_id(username, tweet_id, dynamic_instances=None):
    """æ ¹æ®ç”¨æˆ·åå’Œæ¨æ–‡ ID æŠ“å–æŒ‡å®šæ¨æ–‡"""
    instances = dynamic_instances if dynamic_instances else NITTER_INSTANCES.copy()
    random.shuffle(instances)
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        
        for instance in instances:
            try:
                context = browser.new_context(
                    user_agent=get_random_user_agent(),
                    viewport={'width': 1280, 'height': 720}
                )
                page = context.new_page()
                stealth_sync(page)
                
                # æ„é€ æ¨æ–‡ URL: instance/username/status/tweet_id
                url = f"{instance.rstrip('/')}/{username}/status/{tweet_id}"
                
                print(f"[{username}/{tweet_id}] æ­£åœ¨åŠ è½½: {url}")
                
                try:
                    response = page.goto(url, wait_until="networkidle", timeout=45000)
                    if response and response.status == 403:
                        print(f"[{username}/{tweet_id}] è®¿é—® {instance} è¢«æ‹’ (403 Forbidden)")
                        context.close()
                        continue
                except Exception as e:
                    print(f"[{username}/{tweet_id}] åŠ è½½ {instance} è¶…æ—¶æˆ–å¤±è´¥: {e}")
                    context.close()
                    continue
                
                # æ™ºèƒ½ç­‰å¾…æµè§ˆå™¨éªŒè¯
                challenge_keywords = ["Verifying your browser", "Just a moment", "Checking your browser"]
                for i in range(5):
                    content = page.content()
                    if any(kw in content for kw in challenge_keywords):
                        print(f"[{username}/{tweet_id}] æ£€æµ‹åˆ°æµè§ˆå™¨éªŒè¯ ({i+1}/5)ï¼Œå°è¯•ç­‰å¾…...")
                        page.wait_for_timeout(5000)
                    else:
                        break
                
                soup = BeautifulSoup(page.content(), 'html.parser')
                
                # æŸ¥æ‰¾ä¸»æ¨æ–‡å†…å®¹
                main_tweet = soup.select_one('.main-tweet')
                if not main_tweet:
                    print(f"[{username}/{tweet_id}] åœ¨ {instance} ä¸Šæœªæ‰¾åˆ°æ¨æ–‡")
                    context.close()
                    continue
                
                print(f"[{username}/{tweet_id}] âœ… ä½¿ç”¨å®ä¾‹: {instance}")
                
                # æå–æ¨æ–‡ä¿¡æ¯ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
                content_el = main_tweet.select_one('.tweet-content')
                date_el = main_tweet.select_one('.tweet-date a')
                author_el = main_tweet.select_one('.username')
                
                if not content_el:
                    print(f"[{username}/{tweet_id}] æ¨æ–‡å†…å®¹ä¸ºç©º")
                    context.close()
                    continue
                
                # æå–å›¾ç‰‡
                images = []
                img_els = main_tweet.select('.attachment.image img, .tweet-image img, .still-image img, .attachments img')
                for img in img_els:
                    if any(c in str(img.parent.get('class', [])) for c in ['avatar', 'profile']):
                        continue
                    src = img.get('src', '')
                    if src:
                        if src.startswith('//'):
                            full_src = 'https:' + src
                        elif src.startswith('/'):
                            full_src = instance.rstrip('/') + src
                        else:
                            full_src = src
                        full_src = get_original_image_url(full_src)
                        if 'emoji' not in src.lower():
                            images.append(full_src)
                
                # æå–è§†é¢‘
                video_url = None
                try:
                    video_tag = main_tweet.select_one('video')
                    
                    if video_tag:
                        # æ–¹æ³•1: æ£€æŸ¥ data-url å±æ€§ï¼ˆNitter çš„ä¸»è¦æ–¹å¼ï¼‰
                        data_url = video_tag.get('data-url', '')
                        if data_url:
                            # data-url å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„æˆ–åŒ…å«ç¼–ç çš„ URL
                            if data_url.startswith('/video/'):
                                # æ ¼å¼: /video/ID/https%3A%2F%2F...
                                # æå–å®é™…çš„è§†é¢‘ URL
                                parts = data_url.split('/', 3)
                                if len(parts) > 3:
                                    from urllib.parse import unquote
                                    encoded_url = parts[3]
                                    video_url = unquote(encoded_url)
                                    print(f"[{username}/{tweet_id}] æ‰¾åˆ°è§†é¢‘ (data-url): {video_url[:80]}...")
                            elif data_url.startswith('//'):
                                video_url = 'https:' + data_url
                                print(f"[{username}/{tweet_id}] æ‰¾åˆ°è§†é¢‘ (data-url): {video_url[:80]}...")
                            elif data_url.startswith('/'):
                                video_url = instance.rstrip('/') + data_url
                                print(f"[{username}/{tweet_id}] æ‰¾åˆ°è§†é¢‘ (data-url): {video_url[:80]}...")
                            else:
                                video_url = data_url
                                print(f"[{username}/{tweet_id}] æ‰¾åˆ°è§†é¢‘ (data-url): {video_url[:80]}...")
                        
                        # æ–¹æ³•2: æ£€æŸ¥ src å±æ€§
                        if not video_url:
                            v_src = video_tag.get('src', '')
                            if v_src:
                                if v_src.startswith('//'):
                                    video_url = 'https:' + v_src
                                elif v_src.startswith('/'):
                                    video_url = instance.rstrip('/') + v_src
                                else:
                                    video_url = v_src
                                print(f"[{username}/{tweet_id}] æ‰¾åˆ°è§†é¢‘ (src): {video_url[:80]}...")
                        
                        # æå–å°é¢å›¾
                        poster = video_tag.get('poster', '')
                        if poster:
                            if poster.startswith('//'):
                                full_poster = 'https:' + poster
                            elif poster.startswith('/'):
                                full_poster = instance.rstrip('/') + poster
                            else:
                                full_poster = poster
                            full_poster = get_original_image_url(full_poster)
                            
                            # éªŒè¯å°é¢å›¾æ˜¯å¦å¯è®¿é—®
                            if check_url_accessibility(full_poster):
                                if full_poster not in images:
                                    images.append(full_poster)
                                    poster_added = True
                            else:
                                print(f"[{username}/{tweet_id}] âš ï¸ å°é¢å›¾æ— æ³•è®¿é—®ï¼Œè·³è¿‡: {full_poster}")
                    
                    # æ–¹æ³•3: æ£€æŸ¥ video source æ ‡ç­¾
                    if not video_url:
                        video_source = main_tweet.select_one('video source')
                        if video_source:
                            v_src = video_source.get('src', '')
                            if v_src:
                                if v_src.startswith('//'):
                                    video_url = 'https:' + v_src
                                elif v_src.startswith('/'):
                                    video_url = instance.rstrip('/') + v_src
                                else:
                                    video_url = v_src
                                print(f"[{username}/{tweet_id}] æ‰¾åˆ°è§†é¢‘ (source): {video_url[:80]}...")
                    
                    # æ–¹æ³•4: æŸ¥æ‰¾è§†é¢‘é“¾æ¥
                    if not video_url:
                        video_links = main_tweet.select('a[href*=".mp4"], a[href*=".m3u8"]')
                        for link in video_links:
                            href = link.get('href', '')
                            if href and ('.mp4' in href or '.m3u8' in href):
                                if href.startswith('//'):
                                    video_url = 'https:' + href
                                elif href.startswith('/'):
                                    video_url = instance.rstrip('/') + href
                                else:
                                    video_url = href
                                print(f"[{username}/{tweet_id}] æ‰¾åˆ°è§†é¢‘ (link): {video_url[:80]}...")
                                break
                    
                    # [é€šç”¨é€»è¾‘] å¦‚æœæ²¡æœ‰å°é¢å›¾ï¼Œä¸”æœ‰è§†é¢‘é“¾æ¥ï¼Œå°è¯•ç”Ÿæˆ
                    if 'poster_added' not in locals():
                        poster_added = False
                        
                    if not poster_added and video_url:
                         # å†æ¬¡æ£€æŸ¥ images åˆ—è¡¨
                        if not images:
                            print(f"[{username}/{tweet_id}] âš ï¸ è§†é¢‘æ²¡æœ‰å°é¢å›¾ï¼Œå°è¯•ç”Ÿæˆ...")
                            generated_poster = extract_video_frame(video_url)
                            if generated_poster:
                                if generated_poster not in images:
                                    images.append(generated_poster)
                                print(f"[{username}/{tweet_id}] âœ… è§†é¢‘å°é¢ç”ŸæˆæˆåŠŸ: {generated_poster}")
                    
                    if not video_url:
                        # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘æŒ‡ç¤ºå™¨
                        has_video = main_tweet.select_one('.video-container, .video-overlay, video')
                        if has_video:
                            print(f"[{username}/{tweet_id}] æ£€æµ‹åˆ°è§†é¢‘ä½†æœªèƒ½æå– URL")
                            
                except Exception as e:
                    print(f"[{username}/{tweet_id}] è§†é¢‘æå–å¼‚å¸¸: {e}")
                
                tweet_data = {
                    'content': content_el.get_text(strip=True),
                    'link': url,
                    'published': date_el.get('title', '') if date_el else 'Unknown Time',
                    'author': author_el.get_text(strip=True) if author_el else username,
                    'guid': tweet_id,
                    'is_retweet': False,
                    'images': images,
                    'video_url': video_url
                }
                
                # è¾“å‡ºæå–æ‘˜è¦
                print(f"[{username}/{tweet_id}] " + "=" * 60)
                print(f"[{username}/{tweet_id}] ğŸ“Š æå–æ‘˜è¦:")
                print(f"[{username}/{tweet_id}]   - å†…å®¹: {tweet_data['content'][:50]}...")
                print(f"[{username}/{tweet_id}]   - å›¾ç‰‡: {len(images)} å¼ ")
                if video_url:
                    print(f"[{username}/{tweet_id}]   - è§†é¢‘: âœ… {video_url[:80]}...")
                else:
                    print(f"[{username}/{tweet_id}]   - è§†é¢‘: âŒ æœªæ‰¾åˆ°")
                print(f"[{username}/{tweet_id}] " + "=" * 60)
                
                context.close()
                browser.close()
                return tweet_data
                
            except Exception as e:
                print(f"[{username}/{tweet_id}] è®¿é—® {instance} å‡ºé”™: {e}")
                continue
        
        browser.close()
    return None

def get_tweets_needing_repair():
    """æŸ¥è¯¢æ•°æ®åº“ä¸­éœ€è¦ä¿®å¤å°é¢çš„æ¨æ–‡ (åŒ…å« name=small çš„å›¾ç‰‡)"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # æŸ¥æ‰¾ images å­—æ®µä¸­åŒ…å« 'name=small' çš„è®°å½•
        # æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ–‡æœ¬åŒ¹é…ï¼Œé€‚ç”¨äº JSONB è½¬æ–‡æœ¬åçš„æŸ¥è¯¢
        cursor.execute("""
            SELECT tweet_id, author, images 
            FROM tweets 
            WHERE images::text LIKE '%name=small%';
        """)
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'tweet_id': row[0],
                'username': row[1],
                'images': row[2]
            })
            
        cursor.close()
        conn.close()
        return results
    except Exception as e:
        print(f"[æ•°æ®åº“] âŒ æŸ¥è¯¢å¾…ä¿®å¤æ¨æ–‡å¤±è´¥: {e}")
        return []

def main():
    print(f"[{datetime.now()}] å¯åŠ¨ Colorful State ç›‘æ§ç³»ç»Ÿ...")
    
    # ä»æœ¬åœ°ç¼“å­˜åŠ è½½å¯ç”¨å®ä¾‹
    instances = load_instances()

    # æ£€æŸ¥ä¿®å¤æ¨¡å¼
    repair_mode = os.environ.get('REPAIR_MODE', 'false').lower() == 'true'
    if repair_mode:
        print(f"\n[ç³»ç»Ÿ] ğŸ”§ å¯åŠ¨ä¿®å¤æ¨¡å¼ (REPAIR_MODE)")
        tweets_to_repair = get_tweets_needing_repair()
        
        if not tweets_to_repair:
            print("[ä¿®å¤] æ²¡æœ‰å‘ç°éœ€è¦ä¿®å¤çš„æ¨æ–‡ (æ²¡æœ‰åŒ…å« name=small çš„å›¾ç‰‡)")
            return
            
        print(f"[ä¿®å¤] å‘ç° {len(tweets_to_repair)} æ¡æ¨æ–‡åŒ…å«ä½æ¸…å›¾ç‰‡ï¼Œå¼€å§‹ä¿®å¤...")
        
        for i, info in enumerate(tweets_to_repair):
            print(f"\n--- æ­£åœ¨ä¿®å¤ ({i+1}/{len(tweets_to_repair)}): {info['username']}/{info['tweet_id']} ---")
            try:
                # é‡æ–°æŠ“å–å¹¶ä¿å­˜ï¼ˆsave_tweet_to_db ä¼šå¤„ç†æ›´æ–°ï¼‰
                tweet = scrape_tweet_by_id(info['username'], info['tweet_id'], instances)
                if tweet:
                    save_tweet_to_db(tweet)
                    print(f"[ä¿®å¤] âœ… ä¿®å¤æˆåŠŸ")
                else:
                    print(f"[ä¿®å¤] âš ï¸ ä¿®å¤å¤±è´¥: æ— æ³•é‡æ–°æŠ“å–")
            except Exception as e:
                print(f"[ä¿®å¤] âŒ å¤„ç†å¼‚å¸¸: {e}")
                
        print("\n[ç³»ç»Ÿ] ä¿®å¤ä»»åŠ¡å®Œæˆï¼Œé€€å‡ºã€‚")
        return

    while True:
        cycle_start = time.time()
        print(f"\n--- å¯åŠ¨æ–°ä¸€è½®ç›‘æ§è½®è¯¢ [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ---")
        
        # ä¼˜å…ˆå¤„ç†æ–‡ä»¶ä¸­çš„æ¨æ–‡ URL
        tweet_urls = load_tweet_urls_from_file('tweets.txt')
        
        if tweet_urls:
            print(f"\n[æ¨¡å¼] å•æ¡æ¨æ–‡æŠ“å–æ¨¡å¼")
            
            # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶é‡æ–°æŠ“å–
            force_rescrape = os.environ.get('FORCE_RESCRAPE', 'false').lower() == 'true'
            
            if force_rescrape:
                print(f"[ç³»ç»Ÿ] âš ï¸  å¼ºåˆ¶é‡æ–°æŠ“å–æ¨¡å¼å¼€å¯: å°†é‡æ–°å¤„ç†æ‰€æœ‰ {len(tweet_urls)} æ¡æ¨æ–‡")
                # è§£ææ‰€æœ‰ URL ä½†ä¸æ£€æŸ¥æ•°æ®åº“çŠ¶æ€ï¼Œç›´æ¥è§†ä¸ºå¾…å¤„ç†
                pending_urls = tweet_urls
                parsed_tweets = []
                for url in pending_urls:
                    parsed = parse_tweet_url(url)
                    if parsed:
                        parsed_tweets.append(parsed)
                pending = parsed_tweets
                scraped = [] # å‡è£…æ²¡æœ‰å·²æŠ“å–çš„
            else:
                # æ­£å¸¸æ£€æŸ¥çŠ¶æ€
                scraped, pending = check_tweet_status(tweet_urls)
            
            # æ‰“å°æŠ¥å‘Š
            print_status_report(scraped, pending)
            
            # ä»…æŠ“å–å¾…æŠ“å–çš„æ¨æ–‡
            if pending:
                print(f"[å¼€å§‹æŠ“å–] æŠ“å– {len(pending)} æ¡å¾…æŠ“å–æ¨æ–‡...\n")
                for tweet_info in pending:
                    try:
                        tweet = scrape_tweet_by_id(
                            tweet_info['username'],
                            tweet_info['tweet_id'],
                            instances
                        )
                        if tweet:
                            save_tweet_to_db(tweet)
                    except Exception as e:
                        print(f"[{tweet_info['url']}] å¤„ç†å¼‚å¸¸: {e}")
            else:
                print("[å®Œæˆ] æ‰€æœ‰é…ç½®çš„æ¨æ–‡éƒ½å·²æŠ“å–ï¼Œæ— éœ€é‡å¤æŠ“å–ã€‚")
        
        # å¤„ç†ç”¨æˆ·ç›‘æ§æ¨¡å¼
        if USERS:
            print(f"\n[æ¨¡å¼] ç”¨æˆ·ç›‘æ§æ¨¡å¼ ({len(USERS)} ä¸ªç”¨æˆ·)")
            for target in USERS:
                try:
                    tweet = scrape_nitter_with_playwright(target, instances)
                    if tweet:
                        save_tweet_to_db(tweet)
                    else:
                        print(f"[{target}] æœªèƒ½æŠ“å–åˆ°æ¨æ–‡")
                except Exception as e:
                    print(f"[{target}] å¤„ç†å¼‚å¸¸: {e}")

        if not LOOP_MODE:
            print("\n[ç³»ç»Ÿ] éå¾ªç¯æ¨¡å¼ï¼Œä»»åŠ¡ç»“æŸã€‚")
            break
        
        # è®¡ç®—éœ€è¦ sleep çš„æ—¶é—´
        elapsed = time.time() - cycle_start
        sleep_time = max(10, INTERVAL - elapsed)
        print(f"--- è½®è¯¢ç»“æŸã€‚è€—æ—¶ {elapsed:.1f}sï¼Œå‡†å¤‡ä¼‘çœ  {sleep_time:.1f}s ---\n")
        time.sleep(sleep_time)

if __name__ == "__main__":
    main()
